# å¤ªé™½å…‰ãƒ‘ãƒãƒ«è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ  ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ / Solar Panel Calculation System Maintenance Guide

## ğŸ“‹ æ¦‚è¦ / Overview

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€å¤ªé™½å…‰ãƒ‘ãƒãƒ«é…ç½®è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã®ç¶™ç¶šçš„ãªé‹ç”¨ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã«é–¢ã™ã‚‹ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚

This document provides guidelines for the continuous operation and maintenance of the solar panel layout calculation system.

## ğŸ—“ï¸ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« / Maintenance Schedule

### æ—¥æ¬¡ / Daily
- [ ] ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèª
- [ ] ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
- [ ] APIå¿œç­”æ™‚é–“ç›£è¦–
- [ ] ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯

### é€±æ¬¡ / Weekly
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°ç¢ºèª

### æœˆæ¬¡ / Monthly
- [ ] ä¾å­˜é–¢ä¿‚æ›´æ–°
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- [ ] å®¹é‡è¨ˆç”»è¦‹ç›´ã—

### å››åŠæœŸ / Quarterly
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
- [ ] ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
- [ ] ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ

## ğŸ” ç›£è¦–é …ç›® / Monitoring Items

### 1. ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ / System Health

#### CPUä½¿ç”¨ç‡ç›£è¦–
```bash
#!/bin/bash
# cpu_monitor.sh
CPU_THRESHOLD=80
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)

if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD" | bc -l) )); then
    echo "WARNING: High CPU usage: ${CPU_USAGE}%"
    # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    curl -X POST "https://hooks.slack.com/..." \
         -d "{\"text\":\"High CPU usage: ${CPU_USAGE}%\"}"
fi
```

#### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
```bash
#!/bin/bash
# memory_monitor.sh
MEMORY_THRESHOLD=85
MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')

if (( $(echo "$MEMORY_USAGE > $MEMORY_THRESHOLD" | bc -l) )); then
    echo "WARNING: High memory usage: ${MEMORY_USAGE}%"
    # ãƒ¡ãƒ¢ãƒªãƒ€ãƒ³ãƒ—ã®å–å¾—
    ps aux --sort=-%mem | head -10 > /tmp/memory_usage.log
fi
```

#### ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç›£è¦–
```bash
#!/bin/bash
# disk_monitor.sh
DISK_THRESHOLD=90
DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | cut -d'%' -f1)

if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
    echo "WARNING: High disk usage: ${DISK_USAGE}%"
    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
    find /tmp -type f -mtime +7 -delete
    find results/logs -name "*.log" -mtime +30 -delete
fi
```

### 2. APIç›£è¦– / API Monitoring

#### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
```bash
#!/bin/bash
# health_check.sh
API_URL="http://localhost:8001/health"
RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL")

if [ "$RESPONSE" != "200" ]; then
    echo "ERROR: API health check failed. Status: $RESPONSE"
    # ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
    systemctl restart panel-calculator
fi
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š
```bash
#!/bin/bash
# response_time_check.sh
API_URL="http://localhost:8001/health"
RESPONSE_TIME=$(curl -w "%{time_total}" -o /dev/null -s "$API_URL")
THRESHOLD=2.0

if (( $(echo "$RESPONSE_TIME > $THRESHOLD" | bc -l) )); then
    echo "WARNING: Slow API response: ${RESPONSE_TIME}s"
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æŸ»ã®é–‹å§‹
    top -bn1 > /tmp/performance_snapshot.log
fi
```

### 3. ãƒ­ã‚°ç›£è¦– / Log Monitoring

#### ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–
```bash
#!/bin/bash
# error_log_monitor.sh
LOG_FILE="panel_calculator.log"
ERROR_COUNT=$(grep -c "ERROR" "$LOG_FILE" | tail -1)
WARNING_COUNT=$(grep -c "WARNING" "$LOG_FILE" | tail -1)

echo "Errors: $ERROR_COUNT, Warnings: $WARNING_COUNT"

# æ–°ã—ã„ã‚¨ãƒ©ãƒ¼ã®æ¤œå‡º
if [ "$ERROR_COUNT" -gt 0 ]; then
    echo "Recent errors found:"
    grep "ERROR" "$LOG_FILE" | tail -5
fi
```

## ğŸ”§ å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä½œæ¥­ / Regular Maintenance Tasks

### 1. ä¾å­˜é–¢ä¿‚ç®¡ç† / Dependency Management

#### æœˆæ¬¡æ›´æ–°æ‰‹é †
```bash
#!/bin/bash
# monthly_update.sh

echo "=== Monthly Dependency Update ==="

# 1. ç¾åœ¨ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pip freeze > requirements_backup_$(date +%Y%m%d).txt

# 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯
pip audit

# 3. æ›´æ–°å¯èƒ½ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç¢ºèª
pip list --outdated

# 4. é‡è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ›´æ–°
pip install --upgrade opencv-python numpy scipy flask requests

# 5. ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
python -m pytest tests/ -v

# 6. æ›´æ–°å¾Œã®ä¾å­˜é–¢ä¿‚ã‚’è¨˜éŒ²
pip freeze > requirements_updated_$(date +%Y%m%d).txt

echo "Update completed. Please review test results."
```

#### ä¾å­˜é–¢ä¿‚ã®è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³
```bash
#!/bin/bash
# security_scan.sh

echo "=== Security Vulnerability Scan ==="

# 1. pip auditå®Ÿè¡Œ
pip audit --format=json > security_audit_$(date +%Y%m%d).json

# 2. çµæœã®è§£æ
VULNERABILITIES=$(pip audit --format=json | jq '.vulnerabilities | length')

if [ "$VULNERABILITIES" -gt 0 ]; then
    echo "WARNING: $VULNERABILITIES vulnerabilities found"
    pip audit
    
    # ç·Šæ€¥åº¦ã®é«˜ã„è„†å¼±æ€§ã®ç¢ºèª
    HIGH_SEVERITY=$(pip audit --format=json | jq '.vulnerabilities[] | select(.severity == "high") | length')
    if [ "$HIGH_SEVERITY" -gt 0 ]; then
        echo "CRITICAL: High severity vulnerabilities detected!"
        # ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    fi
fi
```

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† / Database & File Management

#### ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
```bash
#!/bin/bash
# log_rotation.sh

LOG_DIR="results/logs"
ARCHIVE_DIR="$LOG_DIR/archive"
RETENTION_DAYS=90

# ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p "$ARCHIVE_DIR"

# 30æ—¥ä»¥ä¸Šå¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
find "$LOG_DIR" -name "*.log" -mtime +30 -not -path "$ARCHIVE_DIR/*" | while read logfile; do
    filename=$(basename "$logfile")
    gzip "$logfile"
    mv "${logfile}.gz" "$ARCHIVE_DIR/"
    echo "Archived: $filename"
done

# 90æ—¥ä»¥ä¸Šå¤ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å‰Šé™¤
find "$ARCHIVE_DIR" -name "*.gz" -mtime +$RETENTION_DAYS -delete

echo "Log rotation completed"
```

#### çµæœãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
```bash
#!/bin/bash
# cleanup_results.sh

RESULTS_DIR="results"
TEMP_DIR="/tmp/panel_calc"
RETENTION_DAYS=30

echo "=== Results Data Cleanup ==="

# 1. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
rm -rf "$TEMP_DIR"/*

# 2. å¤ã„å¯è¦–åŒ–ç”»åƒã®å‰Šé™¤
find "$RESULTS_DIR/visualizations" -name "*.png" -mtime +$RETENTION_DAYS -delete

# 3. å¤ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
find "$RESULTS_DIR/csv_data" -name "*.csv" -mtime +$RETENTION_DAYS | while read csvfile; do
    gzip "$csvfile"
    echo "Compressed: $(basename "$csvfile")"
done

# 4. ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã®ç¢ºèª
echo "Current disk usage:"
du -sh "$RESULTS_DIR"/*
```

### 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ– / Performance Optimization

#### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
```bash
#!/bin/bash
# db_optimization.sh

echo "=== Database Optimization ==="

# SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã®æœ€é©åŒ–ä¾‹
if [ -f "panel_calc.db" ]; then
    echo "Optimizing SQLite database..."
    sqlite3 panel_calc.db "VACUUM;"
    sqlite3 panel_calc.db "ANALYZE;"
    echo "Database optimization completed"
fi

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰
# sqlite3 panel_calc.db "REINDEX;"
```

#### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
```bash
#!/bin/bash
# cache_cleanup.sh

echo "=== Cache Cleanup ==="

# 1. Pythonã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null
find . -name "*.pyc" -delete

# 2. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
rm -rf /tmp/panel_calc_*

# 3. ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèªï¼ˆLinuxï¼‰
if command -v sync >/dev/null 2>&1; then
    sync
    echo 3 > /proc/sys/vm/drop_caches 2>/dev/null || true
fi

echo "Cache cleanup completed"
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦– / Performance Monitoring

### 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
```python
#!/usr/bin/env python3
# benchmark_test.py

import time
import psutil
import requests
import numpy as np
from datetime import datetime

def benchmark_api_performance():
    """API ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    url = "http://localhost:8001/health"
    response_times = []
    
    print("=== API Performance Benchmark ===")
    
    for i in range(10):
        start_time = time.time()
        response = requests.get(url)
        end_time = time.time()
        
        response_time = end_time - start_time
        response_times.append(response_time)
        
        print(f"Request {i+1}: {response_time:.3f}s (Status: {response.status_code})")
    
    avg_time = np.mean(response_times)
    max_time = np.max(response_times)
    min_time = np.min(response_times)
    
    print(f"\nResults:")
    print(f"Average: {avg_time:.3f}s")
    print(f"Min: {min_time:.3f}s")
    print(f"Max: {max_time:.3f}s")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ã®ãƒã‚§ãƒƒã‚¯
    if avg_time > 1.0:
        print("WARNING: Average response time exceeds 1 second")
    
    return {
        'timestamp': datetime.now().isoformat(),
        'avg_response_time': avg_time,
        'max_response_time': max_time,
        'min_response_time': min_time
    }

def benchmark_calculation_performance():
    """è¨ˆç®—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    from geometry import calculate_panel_layout_fast
    
    print("=== Calculation Performance Benchmark ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    mask_sizes = [(400, 500), (800, 1000), (1200, 1500)]
    panel_size = (50, 80)
    
    results = []
    
    for size in mask_sizes:
        mask = np.ones(size, dtype=np.uint8) * 255
        
        start_time = time.time()
        count, positions = calculate_panel_layout_fast(mask, panel_size[0], panel_size[1])
        end_time = time.time()
        
        calc_time = end_time - start_time
        
        print(f"Size {size}: {calc_time:.3f}s ({count} panels)")
        
        results.append({
            'size': size,
            'calculation_time': calc_time,
            'panel_count': count
        })
    
    return results

if __name__ == "__main__":
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±
    print(f"CPU Count: {psutil.cpu_count()}")
    print(f"Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"Python Version: {sys.version}")
    print()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    api_results = benchmark_api_performance()
    calc_results = benchmark_calculation_performance()
    
    # çµæœã®ä¿å­˜
    with open(f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
        json.dump({
            'api_performance': api_results,
            'calculation_performance': calc_results
        }, f, indent=2)
```

### 2. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç›£è¦–
```bash
#!/bin/bash
# resource_monitor.sh

MONITOR_DURATION=300  # 5åˆ†é–“
INTERVAL=10          # 10ç§’é–“éš”

echo "=== Resource Monitoring (${MONITOR_DURATION}s) ==="

# ãƒ˜ãƒƒãƒ€ãƒ¼å‡ºåŠ›
echo "Timestamp,CPU%,Memory%,DiskIO,NetworkIO" > resource_usage.csv

for ((i=0; i<$((MONITOR_DURATION/INTERVAL)); i++)); do
    TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
    CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
    
    # ãƒ‡ã‚£ã‚¹ã‚¯I/Oï¼ˆç°¡æ˜“ç‰ˆï¼‰
    DISK_IO=$(iostat -d 1 1 | tail -n +4 | awk '{sum+=$4} END {print sum}')
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯I/Oï¼ˆç°¡æ˜“ç‰ˆï¼‰
    NETWORK_IO=$(cat /proc/net/dev | grep eth0 | awk '{print $2+$10}')
    
    echo "$TIMESTAMP,$CPU_USAGE,$MEMORY_USAGE,$DISK_IO,$NETWORK_IO" >> resource_usage.csv
    
    sleep $INTERVAL
done

echo "Resource monitoring completed. Results saved to resource_usage.csv"
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ / Security Maintenance

### 1. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
```bash
#!/bin/bash
# security_audit.sh

echo "=== Security Audit ==="

# 1. ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ãƒã‚§ãƒƒã‚¯
echo "Checking file permissions..."
find . -type f -name "*.py" -not -perm 644 -ls
find . -type d -not -perm 755 -ls

# 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
echo "Checking configuration security..."
grep -r "password\|secret\|key" *.py *.yml *.json 2>/dev/null | grep -v "example"

# 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
echo "Checking open ports..."
netstat -tulpn | grep LISTEN

# 4. ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚§ãƒƒã‚¯
echo "Checking running processes..."
ps aux | grep python

echo "Security audit completed"
```

### 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒªã‚¹ãƒˆã‚¢
```bash
#!/bin/bash
# backup_system.sh

BACKUP_DIR="/backup/panel_calc"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="panel_calc_backup_$DATE"

echo "=== System Backup ==="

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p "$BACKUP_DIR"

# 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
tar -czf "$BACKUP_DIR/${BACKUP_NAME}_app.tar.gz" \
    --exclude="__pycache__" \
    --exclude="*.pyc" \
    --exclude="results/logs" \
    --exclude="results/visualizations" \
    *.py requirements.txt *.yml *.md

# 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
tar -czf "$BACKUP_DIR/${BACKUP_NAME}_config.tar.gz" \
    docker-compose*.yml \
    requirements.txt \
    *.env 2>/dev/null || true

# 3. é‡è¦ãªçµæœãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
tar -czf "$BACKUP_DIR/${BACKUP_NAME}_data.tar.gz" \
    results/csv_data \
    results/reports

# 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼
echo "Verifying backups..."
for backup_file in "$BACKUP_DIR"/${BACKUP_NAME}_*.tar.gz; do
    if tar -tzf "$backup_file" >/dev/null 2>&1; then
        echo "âœ“ $(basename "$backup_file") - OK"
    else
        echo "âœ— $(basename "$backup_file") - FAILED"
    fi
done

# 5. å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šï¼‰
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +30 -delete

echo "Backup completed: $BACKUP_NAME"
```

## ğŸ“ˆ å®¹é‡è¨ˆç”» / Capacity Planning

### ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡äºˆæ¸¬
```python
#!/usr/bin/env python3
# capacity_planning.py

import os
import json
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

def analyze_disk_usage():
    """ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã®åˆ†æã¨äºˆæ¸¬"""
    
    # éå»ã®ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¾‹ï¼‰
    usage_data = []
    
    # ç¾åœ¨ã®ä½¿ç”¨é‡
    total, used, free = shutil.disk_usage(".")
    current_usage = used / total * 100
    
    print(f"Current disk usage: {current_usage:.1f}%")
    print(f"Free space: {free / (1024**3):.1f} GB")
    
    # æˆé•·ç‡ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    daily_growth_gb = 0.1  # 1æ—¥ã‚ãŸã‚Š100MBã®æˆé•·ã¨ä»®å®š
    
    # 30æ—¥å¾Œã®äºˆæ¸¬
    days_ahead = 30
    predicted_usage = current_usage + (daily_growth_gb * days_ahead * 1024**3 / total * 100)
    
    print(f"Predicted usage in {days_ahead} days: {predicted_usage:.1f}%")
    
    if predicted_usage > 80:
        print("WARNING: Disk usage will exceed 80% in 30 days")
        print("Consider implementing data archival or increasing storage")
    
    return {
        'current_usage_percent': current_usage,
        'free_space_gb': free / (1024**3),
        'predicted_usage_percent': predicted_usage,
        'days_until_80_percent': max(0, (80 - current_usage) / (daily_growth_gb * 1024**3 / total * 100))
    }

if __name__ == "__main__":
    results = analyze_disk_usage()
    
    # çµæœã®ä¿å­˜
    with open(f"capacity_analysis_{datetime.now().strftime('%Y%m%d')}.json", 'w') as f:
        json.dump(results, f, indent=2)
```

## ğŸ“ ç·Šæ€¥æ™‚å¯¾å¿œ / Emergency Response

### ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§æ‰‹é †
```bash
#!/bin/bash
# emergency_recovery.sh

echo "=== Emergency Recovery Procedure ==="

# 1. ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ã®ç¢ºèª
echo "Checking service status..."
systemctl status panel-calculator || echo "Service not running"

# 2. ãƒ­ã‚°ã®ç¢ºèª
echo "Checking recent errors..."
tail -50 panel_calculator.log | grep -E "(ERROR|CRITICAL)"

# 3. ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã®ç¢ºèª
echo "Checking system resources..."
df -h
free -h
top -bn1 | head -20

# 4. è‡ªå‹•å¾©æ—§ã®è©¦è¡Œ
echo "Attempting automatic recovery..."

# ãƒ—ãƒ­ã‚»ã‚¹ã®å†èµ·å‹•
pkill -f "python.*api_integration"
sleep 5
python api_integration.py &

# 5. å¾©æ—§ç¢ºèª
sleep 10
if curl -s http://localhost:8001/health >/dev/null; then
    echo "âœ“ Service recovered successfully"
else
    echo "âœ— Manual intervention required"
    echo "Please check logs and contact support"
fi
```

---

**æœ€çµ‚æ›´æ–° / Last Updated**: 2025-07-02  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³ / Version**: 1.2.0  
**æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼ / Next Review**: 2025-10-02
